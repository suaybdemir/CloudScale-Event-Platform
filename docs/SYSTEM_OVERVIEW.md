# Technical Overview: Resilient Event Processing Pipeline

![System Architecture Diagram](images/system_architecture_diagram.png)

**Project Identity:** This is a high-throughput Distributed Event Processing system designed for **Resilience**, **Traceability**, and **Heuristic Risk Analysis**. While implemented as a Fraud Detection prototype, the core value lies in its architectural trade-offs and production-grade reliability patterns.

---

## üèóÔ∏è System Architecture

The system follows a decoupled, asynchronous architecture using the **Competing Consumers** and **Pipes and Filters** patterns.

### 1. Ingestion Layer (Edge Enrichment v2.1)
- **Ingestion API**: Optimized for low latency. Performs synchronous validation (FluentValidation) before queueing.
- **Trusted Proxy Enforcement**: Extracts Client IP via a CIDR-aware `X-Forwarded-For` chain analysis. It identifies the rightmost non-trusted proxy to prevent IP spoofing.
- **Security Context Fingerprinting**: Generates a periodic `ContextHash` using `Validated-IP + DeviceId + UserAgent` to detect session hijacking or automated replay.

### 2. Messaging & Idempotency Stability
- **Secure Deduplication (Payload Hashing)**: Idempotency is verified in two steps:
    1. Check if `EventId` exists.
    2. Compare the recorded `PayloadHash` with the new request's hash.
- **Collision Detection**: If IDs match but hashes differ, the system flags an **Idempotency Collision** (potential tampering or replay attack) and returns `409 Conflict`.
- **Resilient Persistence**: Powered by a **Polly-based Retry Pipeline** in the Event Processor, handling transient Cosmos DB throttling (429) and timeouts.
- **Dead Letter Queue (DLQ)**: Terminal failures are captured with **Forensic Metadata** (ErrorType, FailedAt, RetryCount) for automated triage.
- **Strict Side-Effect Ordering**:
    - **Persistence-First Boundary**: To prevent "Phantom Alerts" (alerting on data that failed to persist), the system enforces a strict boundary. No external side-effect (Email, Metric Increment, Downstream Event) is executed until the Cosmos DB `CreateItemAsync` call returns a generic `200 OK` or `409 Conflict`.
    - **Atomicity**: If persistence fails, the process crashes/retries *before* emitting any signal. This ensures that **every alert observed by a human is backed by durable data on disk.**

### 3. Intelligence & Risk Engine (v2.1 Refinements)
- **Sigmoid-Floor Confidence Scoring**: Confidence starts at **50% (0.5)** for new entities and scales to 100% via a dampening function as history accumulates.
- **Confidence Bypass**: Critical heuristics (e.g., Impossible Travel > 60) bypass the multiplier to ensure high-risk anomalies are blocked regardless of account age.
- **Temporal State & Late Arrivals**:
    - Scoring uses **Occurrence Time**, not arrival time.
    - If a late event arrives, the engine **re-hydrates** the state window for that period and **re-calculates** risk.
    - Significant shifts in historical risk scores trigger **"Late Detected Fraud"** alerts.
- **Hybrid Scoring Matrix**:
    - **Weighted Composite**: Velocity (40%), Geo-Travel (40%), and Device Profile (20%).
    - **Max-Signal Floor**: Prevents dilution; the system triggers if *either* the weighted score *or* any individual raw signal exceeds the threshold.

### 4. Storage & Observability
- **Hybrid Partitioning**: Partitioned by `TenantId:yyyy-MM`. This balances high-throughput write distribution with efficient time-series querying for dashboards.
- **TTL Management**: Events are persisted with a default **30-day TTL** in the hot container, ensuring storage cost efficiency while maintaining recent history for risk lookups.
- **Change Feed Integration**: Designed to use the Cosmos DB Change Feed to populate Read-Optimized models, decoupling ingestion from analytics.
- **Read-Model Resilience & Rehydration**:
    - **Source of Truth**: The system treats the Cosmos DB 'Hot Store' as the immutable Source of Truth. The Dashboard (Read Model) is a disposable projection.
    - **Versioned Rebuilds**: In case of logic bugs or view corruption, we do not patch the data in place. Instead, we deploy a new Read-Model Container (vNext) and trigger a **Change Feed Replay from 'StartFromBeginning'**.
    - **Zero-Downtime Migration**: The system projects events into the new container in parallel. Once the `FeedLag` metric hits zero, the API transparently switches reads to the new version (Blue/Green Data Deployment). This guarantees eventual consistency without stopping the ingestion pipeline.

---

## üõ°Ô∏è Production-Grade Resilience Patterns

| Pattern | Implementation Detail | Benefit |
| :--- | :--- | :--- |
| **Idempotency** | ID + Payload Hash Validation | Prevents Replay Attacks with tampered data. |
| **Observability** | **Synthetic Canary Watchdog** | Detects config drift (e.g., XFF) by injecting known fraud. |
| **Temporal** | Late Arrival Re-calculation | Catches fraudsters hiding behind network latency. |
| **Hardening** | CIDR-Aware XFF Parsing | Defeats IP spoofing behind proxy chains. |
| **Scoring** | Sigmoid-Floor Confidence | Reduces false positives without ignoring early-stage risk. |

---

## üèóÔ∏è Principal Refinement: Deep Resilience

### 1. The Watchdog Mechanism (Canary events)
To prevent "Silent Death" via configuration drift (e.g., an incorrect CIDR list making the system blind), a `Watchdog Service` enjects conscious fraud events from a blacklisted IP every 60 seconds.
- **Mechanism**: If the Engine fails to block these events, the system immediately triggers a **Sev-1 Critical Alert**: "Security Configuration Failure".

### 2. Temporal State Integrity
By decoupling processing time from event time, the system remains immune to "Latency Camouflage". Late events trigger a state re-hydration, ensuring the truth of a user's behavior is eventually consistent and fully audited, even if processed out of order.

---

## üß™ Verification Strategy (Principal Proof)

The system's integrity and performance have been validated through a live, comprehensive verification suite:
- **[x] Near-Real-Time SLA**: Stats API latency measured at **<1ms** response time, well within the 250ms P99 target.
- **[x] Idempotency Protection**: Verified that duplicate events with different payloads trigger an `IDEMPOTENCY COLLISION` critical log, preventing tampering.
- **[x] Temporal Integrity**: Successfully verified that "Late Arrival" events trigger historical state re-hydration and risk re-calculation.
- **[x] Persistence-First Atomicity**: Code audit and log sequencing confirm all side-effects (alerts/scoring) are gated by successful Cosmos DB write operations.
- **[x] Read-Model Rehydration**: Change Feed Projector is active and supports full state rebuild from `StartFromBeginning`.
- **[x] Synthetic Watchdog**: Verified that conscious fraud injection triggers the correct "High Risk" signal path to the dashboard.

---

## üí¨ Focus Areas for Review
1. **Consistency vs. Latency**: Is the "Read-after-Write" verification in the dashboard sufficient for production monitoring?
2. **Partitioning Strategy**: Is `TenantId:Date` the optimal Partition Key for multi-million event scale?
3. **Scoring Weighting**: Are the heuristic weights (40/40/20) balanced enough, or should we shift to a probabilistic model?
